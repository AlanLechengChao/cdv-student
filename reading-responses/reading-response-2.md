## Technology Legitimizes and Reinforces Previously Existed Social Injustices

Of all the examples of biased technological implementation mentioned in the collection of podcasts and videos, one common causation of those problems is that technology is not capable of *fairing out* previously existed social injustices, and sometimes technology is being ignored of magnifying these social injustices. While most people who hold an optimistic view of technology using in automating social services naively believe that an automated process that does not involve human operation will be able to avoid subjective individual biases, an obvious counterargument is the question of how possible it is to build a truly neutral system that can humanistically smooth out the complex and unquantifiable status quo of socio-political power dynamics. In most of the examples, the answer is apparently no, and the imperfections in technology might thus make the existing problems worse, just as what Kate Crawford mentioned during the podcast: **What we used to afraid that computers will be too smart and takes over the world, but the truth is computers are too stupid and they already did**. Virginia Eubanks talked about a case in the podcast, a woman with 2 kids in economic and health crisis is applying for a job, yet the algorithm flagged the woman as not reliable because she did not go to the interview on time, and the system marked it as intentional, eventually she died because of varies reasons. This is a case that reflect what Virginia Eubanks mentioned as a shift from *case management* to a *task based system* in the field of public services, individual workers in the field no longer hold accountable for individual cases of services, but the entire digital system is responsible for that incident, and this process is **automated**, easily neglected by human beings who could have taken actions.

The process of **automation** is often horrifying to think about in social context, because itself is involved in so many ethical crisis, just as what Eubanks quoted in the podcast, "*the state doesn't need a cop to kill a person*" and "*systems act as a kind of empathy-overwrite*". Personally, I don't believe that moral judgements can be quantified as data, just as a data-driven system could be able to properly handle moral judgements. In contrary, state power equipped with technology could alleviate their visibility of conducting **killing behavior**, as in the wildest senses. Eubanks mentioned a lot cases in the Unites States that implemented digital system in public services like child abuse logging, or housing registration. Yet in a lot of cases in the U.S., these data sets collect more data from the poor, where they lack of a private institution for services, and the more they log their data into the system, the more potentially moral accusation could be found. This negative feedback loop is probably not prospected as programmers and government officials built the system. There are similar cases in China. I have seen a video of one of the lead programmer/engineer at Alibaba who is assigned to code the health code system one year ago when the pandemic first started, talking about his experience receiving the orders from **above** to build a nationwide health surveillance system, which includes the cooperation that happened between the enterprise and the state. From his words, I could see that he is passionate about the efforts he and his team put into it within a very short amount of time, and a simplistic prospect of the convenience health code brings to controlling the pandemic in China. One shocking little detail about this is that they even printed out the first line of code of the program, mounted and put it into the national museum. From his words, we can see that the builders of these massive surveillance system have little idea about how much impact on ethical problems do these programs have. Aside from the actual benefits that assists the pandemic control, some people without a proficient access to modern technology, that is mostly the elderly, are excluded from entering the hospital, public transportation, and all sorts of public services.

However, we might never know what ethical standpoint of the people who proposed the usage of massive surveillance systems in social services, or whether they even have a ethical standpoint. Yet the process of automation could indeed draw this ethical question into a more complicated field.
